{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIM Python API-Webscraping workshop: 2020-09-18\n",
    "### Helge Marahrens (hmarahre@iu.edu) & Anne Kavalerchik (akavaler@iu.edu)\n",
    "#### Part 2: Web scraping HTML\n",
    "\n",
    "http://toscrape.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the HTML of a URL we need: [http://quotes.toscrape.com/](http://quotes.toscrape.com/).\n",
    "\n",
    "It's a website with quotations, the people they are attributed to, and the short biographies of those people.\n",
    "\n",
    "We will use the python `requests` library to send HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Response [200]>` means that our request was successful.\n",
    "Usually what we want is the text from a website.\n",
    "Let's get the text and print it. [Compare it to the source code of the actual webpage](view-source:http://quotes.toscrape.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmltext = response.text\n",
    "print(htmltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a combination of regular expressions, string matching, and loops to navigate the html, but luckily the Beautiful Soup package makes it much easier. [BeautifulSoup documentation is here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(htmltext,'html.parser')\n",
    "print(soup) # this doesn't look much different than before we parsed it, but it will let us navigate it easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to navigate this. \n",
    "First start by navigating it using __tag names__.\n",
    "This returns the first element with that tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n",
    "print(soup.head)\n",
    "# title\n",
    "print(soup.title)\n",
    "# body\n",
    "print(soup.body)\n",
    "# h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of data structures are these returning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup))\n",
    "print(type(soup.head))\n",
    "print(type(soup.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually treat bs4.element.Tag as BeautifulSoup and navigate those the same way.\n",
    "Try to get to the tag  `a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.body)\n",
    "print(soup.body.div)\n",
    "print(soup.body.div.div)\n",
    "print(soup.body.div.div.div)\n",
    "print(soup.body.div.div.div.div)\n",
    "print(soup.body.div.div.div.h1.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that doing that was also the same as doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.h1.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the style of that tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.h1.a['style'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `.find` with the tag name and other attributes, and `.findAll` to return __all__ tags fitting those attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the same\n",
    "print(soup.h1)\n",
    "print(soup.find('h1'))\n",
    "print('')\n",
    "print(soup.find(style = \"text-decoration: none\"))\n",
    "print(soup.h1.a)\n",
    "print('')\n",
    "#print(soup.findAll(div))\n",
    "print(len(soup.findAll('div')))\n",
    "print(type(soup.findAll('div')))\n",
    "print(soup.find(''))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice on the first quotation, by Albert Einstein.\n",
    "We get this by going to the first tag that has the class of quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einstein = soup.find('div',{'class':'quote'})\n",
    "print(einstein)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can investigate this tag a bit...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(einstein.div)\n",
    "print(einstein.span)\n",
    "print(einstein.a)\n",
    "print(einstein.findAll('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all of the tags for that quotation, and use `get_text` to get __only__ the text from each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_tags = einstein.findAll('a',{'class':'tag'})\n",
    "e_tags_list = []\n",
    "for e_tag in e_tags:\n",
    "    print(e_tag.get_text())\n",
    "    e_tags_list.append(e_tag.get_text())\n",
    "e_tags_list\n",
    "\n",
    "# We can do the equivalent task without a loop using this line:\n",
    "e_tags_list = [e_tag.get_text() for e_tag in e_tags]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now navigate just to \"Albert Einstein\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einstein\n",
    "einstein.small.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get Albert Einstein's quotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(einstein.span.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a list of every person on this page, and then every quotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_person_tags = soup.findAll('div',{'class':'quote'})\n",
    "for person_tag in all_person_tags:\n",
    "    print(person_tag.small.get_text())\n",
    "    \n",
    "persons = [person_tag.small.get_text() for person in all_person_tags]\n",
    "\n",
    "quotes = [person_tag.span.get_text() for person in all_person_tags]\n",
    "\n",
    "print(persons)\n",
    "print(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say what we really want is to make a big spreadshet of all the names and quotations on this website. This means we need to go through the pages. Let's store everything in a python __dictionary__ before turning it into a spreadsheet with `pandas`.\n",
    "\n",
    "We'll store each entry in this format:\n",
    "`{'Person':'Albert Einstein',\n",
    "'Quotation':'The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.'}`\n",
    "\n",
    "First, let's make a __function__ to do that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storePerson(person_tag):\n",
    "    name = person_tag.small.get_text()\n",
    "    quote = person_tag.span.get_text()\n",
    "    return {'name':name,'quote':quote}\n",
    "\n",
    "print(storePerson(einstein))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through every person/quote on the page, and return a __list__ of __dictionaries__, where every dictionary is composed of 2 __key-value__ pairs: 1) Person's name 2) Person's quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_person_tags = soup.findAll('div',{'class':'quote'})\n",
    "all_quotes = []\n",
    "for person_tag in all_person_tags:\n",
    "    all_quotes.append(storePerson(person_tag))\n",
    "    \n",
    "print(all_quotes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we __really__ want is a list of __every person on this website__. To do this, we need to use `requests` to call on all the pages.\n",
    "\n",
    "It's helpful to do some investigating first. Notice that [quotes.toscrape.com/page/1/](quotes.toscrape.com/page/1/) is this page we have been working with, [quotes.toscrape.com/page/2/](quotes.toscrape.com/page/2/) is the next page, and [quotes.toscrape.com/page/10/](quotes.toscrape.com/page/10/) is the last page. So our goal is to scrape these __10__ pages.\n",
    "\n",
    "We can generate these 10 different URLs like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com/page/'\n",
    "page_num = 1\n",
    "for page_num in range(1,11):\n",
    "    print(page_num)\n",
    "    print(url + str(page_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basically going to repeat the process that we did to get all the information from the first page for all 10 pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_persons_pages = []\n",
    "\n",
    "for page_num in range(1,11):\n",
    "    time.sleep(.5) # So as not to overload the server!\n",
    "    print(url + str(page_num))\n",
    "    response = requests.get(url + str(page_num))\n",
    "    htmltext = response.text\n",
    "    soup = bs(htmltext,'html.parser')\n",
    "    all_person_tags = soup.findAll('div',{'class':'quote'})\n",
    "    for person_tag in all_person_tags:\n",
    "        all_persons_pages.append(storePerson(person_tag))\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! Here is what the resulting dictionary looks like if we print it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_persons_pages))\n",
    "print(all_persons_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this a JSON like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('famous_quotes.json','w') as f:\n",
    "    json.dump(all_persons_pages,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also into a `pandas` DataFrame to export it as an Excel or CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_persons_pages)\n",
    "df\n",
    "df.to_csv('all_quotes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
